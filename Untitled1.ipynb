{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "WARNING:tensorflow:From c:\\users\\konnu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From c:\\users\\konnu\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 155)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 4096)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 155, 300)     1936200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 155, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          570368      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 6454)         1658678     dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,279,870\n",
      "Trainable params: 5,279,870\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import pdb\n",
    "import time\n",
    "import gc\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "with open(\"./data/features/train_features.pkl\", \"rb\") as handle:\n",
    "    train_features = pickle.load(handle)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "with open(\"./data/features/valid_features.pkl\", \"rb\") as handle:\n",
    "    valid_features = pickle.load(handle)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "train_captions = pd.read_csv(\"./data/split_lists/train_ids.csv\", dtype = str)\n",
    "valid_captions = pd.read_csv(\"./data/split_lists/valid_ids.csv\", dtype = str)\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "# does everything make sense, in terms of shapes? \n",
    "print(valid_captions.shape[0] == len(valid_features))\n",
    "print(train_captions.shape[0] == len(train_features))\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "#train_captions.caption = \"startseq \" + train_captions.caption + \" endseq\"\n",
    "#valid_captions.caption = \"startseq \" + valid_captions.caption + \" endseq\"\n",
    "train_captions = train_captions.dropna()\n",
    "valid_captions = valid_captions.dropna()\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "train_captions.head()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "# fit a tokenizer \n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "all_captions = np.concatenate([train_captions.caption.values,valid_captions.caption.values])\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "# fit tokenizer\n",
    "tokenizer.fit_on_texts(all_captions.astype(str))\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "# store the vocabulary size\n",
    "vocab_size = 1 + len(tokenizer.word_index)\n",
    "VOCAB_SIZE = vocab_size\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "with open(\"./tokenizer.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle)\n",
    "\n",
    "\n",
    "# In[]:\n",
    "\n",
    "def encode_and_pad(caption, sequence_length = 15):\n",
    "    # questions encoded as index vectors\n",
    "    encoded = tokenizer.texts_to_sequences([caption])\n",
    "    # padded squences to be of length [sequence_length]\n",
    "    padded = pad_sequences(encoded, \n",
    "                            maxlen = sequence_length,\n",
    "                            padding = \"post\", \n",
    "                            truncating = \"post\")[0]\n",
    "    return(padded)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "def encode(caption):\n",
    "    # questions encoded as index vectors\n",
    "    encoded = tokenizer.texts_to_sequences([caption])[0]\n",
    "    return (encoded)\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# convert a dictionary of {photo_id : photo-featres} pairs and a dataframe of captions into two numpy arrays\n",
    "# that can be used as a consolidated training dataset\n",
    "def consolidate_dataset(photo_id,features_dict, captions_df, sequence_length = 155):\n",
    "    # keep track of the photo features and caption sequenes in lists\n",
    "    X_photos, X_captions = [], []\n",
    "    y = [] # build response vector\n",
    "    e = 0\n",
    "    prevtime = time.time()\n",
    "    current_feature = features_dict\n",
    "    current_caption = str(captions_df.loc[captions_df.photo_id == photo_id].iloc[0][\"caption\"])\n",
    "    current_caption_split = current_caption.split()\n",
    "    for i in range(1,len(current_caption.split())):\n",
    "        # add a copy of the photo features\n",
    "        X_photos.append(current_feature)\n",
    "        # encode the input and output sequence\n",
    "        in_words, out_word = \" \".join(current_caption_split[:i]), current_caption_split[i]\n",
    "        in_seq = encode_and_pad(in_words, sequence_length = sequence_length)\n",
    "        # add the training sequences and responses to list\n",
    "        X_captions.append(in_seq)\n",
    "        out_word = to_categorical([encode(out_word)], num_classes = vocab_size)[0]\n",
    "        y.append(out_word)\n",
    "    # return all three\n",
    "    return np.array(X_photos), np.array(X_captions), np.array(y)\n",
    "                                                   \n",
    "# In[28]:\n",
    "\n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(features_dict, captions_df, sequence_length = 155):\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for photo_id in captions_df['photo_id']:\n",
    "        # if the photo_id is not in the feature dictionary, move on\n",
    "            if photo_id not in features_dict:\n",
    "                continue\n",
    "            current_feature = features_dict[photo_id][0]\n",
    "            X_photos, X_captions, y = consolidate_dataset(photo_id, current_feature, captions_df, sequence_length = 155)\n",
    "            yield ([X_photos, X_captions], y)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "# In[30]:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras import models\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, RepeatVector, TimeDistributed, Masking\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "def load_npy(path):\n",
    "    with open(path, \"rb\") as handle:\n",
    "        arr = np.load(handle)\n",
    "    handle.close()\n",
    "    return (arr)\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "#embedding_matrix = load_npy(\"./embedding_matrix.npy\")\n",
    "\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 300, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "   # plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "model = define_model(VOCAB_SIZE, 155)\n",
    "\n",
    "gen = data_generator(train_features, train_captions, 155)\n",
    "#y, t = next(gen)\n",
    "#print(y[0].shape)\n",
    "#print(y[1].shape)\n",
    "#print(t.shape)\n",
    "\n",
    "\n",
    "# epochs = 2\n",
    "# steps = len(train_captions)\n",
    "# for i in range(0,epochs):\n",
    "#     # create the data generator\n",
    "#     generator1 = data_generator(train_features, train_captions, sequence_length=155)\n",
    "#     # fit for one epoch\n",
    "#     model.fit(generator1, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "#     # save model\n",
    "#     model.save('./fresh_models/model_' + str(i) + '.h5')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8000/8000 [==============================] - 2236s 279ms/step - loss: 5.3425\n",
      "Epoch 1/1\n",
      "2859/8000 [=========>....................] - ETA: 21:07 - loss: 5.1588"
     ]
    }
   ],
   "source": [
    "model = models.load_model(\"./fresh_models/model_1.h5\")\n",
    "epochs = 4\n",
    "steps = len(train_captions)\n",
    "for i in range(2,epochs):\n",
    "    # create the data generator\n",
    "    generator1 = data_generator(train_features, train_captions, sequence_length=155)\n",
    "    # fit for one epoch\n",
    "    model.fit(generator1, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # save model\n",
    "    model.save('./fresh_models/model_' + str(i) + '.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
